{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Multi-Task Emotion Recognition Training\n",
        "ResNet-18 with dual heads: 8-class classification + VA regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell1_header"
      },
      "source": [
        "### 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip install kaggle torch torchvision matplotlib seaborn scikit-learn pandas numpy opencv-python pillow onnx tf2onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell2_header"
      },
      "source": [
        "### 2. Mount Drive & Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Upload your kaggle.json to Google Drive or use Colab files\n",
        "# Then copy it to the right location\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell3_header"
      },
      "source": [
        "### 3. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell3"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d noamsegal/affectnet-training-data\n",
        "!unzip -q affectnet-training-data.zip -d /content/affectnet_data\n",
        "\n",
        "# Alternatively, if using mstjebashazida version:\n",
        "# !kaggle datasets download -d mstjebashazida/affectnet\n",
        "# !unzip -q affectnet.zip -d /content/affectnet_data\n",
        "\n",
        "print(\"Dataset downloaded and extracted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell4_header"
      },
      "source": [
        "### 4. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell5_header"
      },
      "source": [
        "### 5. Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell5"
      },
      "outputs": [],
      "source": [
        "class AffectNetDataset(Dataset):\n",
        "    '''\n",
        "    AffectNet Dataset Loader\n",
        "    \n",
        "    Loads images with categorical emotion labels and continuous valence/arousal values\n",
        "    '''\n",
        "    def __init__(self, data_df, img_dir, transform=None):\n",
        "        '''\n",
        "        Args:\n",
        "            data_df: DataFrame with columns ['image_path', 'emotion', 'valence', 'arousal']\n",
        "            img_dir: Root directory containing images\n",
        "            transform: Optional transforms to apply\n",
        "        '''\n",
        "        self.data_df = data_df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Get image path and load image\n",
        "        img_name = self.data_df.loc[idx, 'image_path']\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        \n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            # Fallback to black image if loading fails\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "        \n",
        "        # Get labels\n",
        "        emotion = int(self.data_df.loc[idx, 'emotion'])\n",
        "        valence = float(self.data_df.loc[idx, 'valence'])\n",
        "        arousal = float(self.data_df.loc[idx, 'arousal'])\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, emotion, torch.tensor([valence, arousal], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell6_header"
      },
      "source": [
        "### 6. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell6"
      },
      "outputs": [],
      "source": [
        "# Update this path based on your dataset structure\n",
        "DATA_DIR = '/content/affectnet_data'\n",
        "\n",
        "# Load annotations CSV\n",
        "# Adjust column names based on actual CSV structure\n",
        "# Expected columns: image_path, emotion, valence, arousal\n",
        "annotations_file = os.path.join(DATA_DIR, 'annotations.csv')  # Update filename\n",
        "\n",
        "# If CSV doesn't exist, create from folder structure\n",
        "# This is a placeholder - adjust based on actual dataset structure\n",
        "def load_affectnet_annotations(data_dir):\n",
        "    '''Load or create annotations DataFrame'''\n",
        "    csv_path = os.path.join(data_dir, 'annotations.csv')\n",
        "    \n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "    else:\n",
        "        # Create annotations from folder structure if needed\n",
        "        # This is dataset-specific - update based on actual structure\n",
        "        print(\"Creating annotations from dataset structure...\")\n",
        "        data_list = []\n",
        "        \n",
        "        # Example: if images are in emotion-named folders\n",
        "        emotion_map = {\n",
        "            'neutral': 0, 'happy': 1, 'sad': 2, 'surprise': 3,\n",
        "            'fear': 4, 'disgust': 5, 'anger': 6, 'contempt': 7\n",
        "        }\n",
        "        \n",
        "        for emotion_name, emotion_id in emotion_map.items():\n",
        "            emotion_dir = os.path.join(data_dir, emotion_name)\n",
        "            if os.path.exists(emotion_dir):\n",
        "                for img_name in os.listdir(emotion_dir):\n",
        "                    if img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                        data_list.append({\n",
        "                            'image_path': os.path.join(emotion_name, img_name),\n",
        "                            'emotion': emotion_id,\n",
        "                            'valence': 0.0,  # Placeholder if not available\n",
        "                            'arousal': 0.0   # Placeholder if not available\n",
        "                        })\n",
        "        \n",
        "        df = pd.DataFrame(data_list)\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Created {len(df)} annotations\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "df = load_affectnet_annotations(DATA_DIR)\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"\\nEmotion distribution:\\n{df['emotion'].value_counts().sort_index()}\")\n",
        "\n",
        "# Train/Val/Test split (70/15/15)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['emotion'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['emotion'])\n",
        "\n",
        "print(f\"\\nTrain: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell7_header"
      },
      "source": [
        "### 7. Transforms & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell7"
      },
      "outputs": [],
      "source": [
        "# ImageNet normalization values\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Training transforms with augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),  # ±15 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Validation/Test transforms (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = AffectNetDataset(train_df, DATA_DIR, transform=train_transform)\n",
        "val_dataset = AffectNetDataset(val_df, DATA_DIR, transform=val_transform)\n",
        "test_dataset = AffectNetDataset(test_df, DATA_DIR, transform=val_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
        "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
        "                       num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
        "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell8_header"
      },
      "source": [
        "### 8. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell8"
      },
      "outputs": [],
      "source": [
        "class MultiTaskEmotionNet(nn.Module):\n",
        "    '''\n",
        "    Multi-Task Emotion Recognition Model\n",
        "    \n",
        "    Architecture:\n",
        "    - Shared ResNet-18 backbone (pretrained on ImageNet)\n",
        "    - Branch 1: Categorical emotion classification (8 classes)\n",
        "    - Branch 2: Valence-Arousal regression (2 continuous values)\n",
        "    '''\n",
        "    def __init__(self, num_classes=8, pretrained=True):\n",
        "        super(MultiTaskEmotionNet, self).__init__()\n",
        "        \n",
        "        # Load pretrained ResNet-18\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        \n",
        "        # Extract feature extractor (remove final FC layer)\n",
        "        self.shared_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        \n",
        "        # Feature dimension from ResNet-18\n",
        "        feature_dim = 512\n",
        "        \n",
        "        # Categorical emotion classification head\n",
        "        self.emotion_classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Valence-Arousal regression head\n",
        "        self.va_regressor = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Tanh()  # Output range [-1, 1]\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Shared feature extraction\n",
        "        features = self.shared_backbone(x)\n",
        "        features = features.view(features.size(0), -1)  # Flatten\n",
        "        \n",
        "        # Multi-task outputs\n",
        "        emotion_logits = self.emotion_classifier(features)\n",
        "        va_output = self.va_regressor(features)\n",
        "        \n",
        "        return emotion_logits, va_output\n",
        "\n",
        "# Initialize model\n",
        "model = MultiTaskEmotionNet(num_classes=8, pretrained=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell9_header"
      },
      "source": [
        "### 9. Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell9"
      },
      "outputs": [],
      "source": [
        "class MultiTaskLoss(nn.Module):\n",
        "    '''\n",
        "    Combined loss for multi-task learning\n",
        "    \n",
        "    L_total = alpha * CrossEntropyLoss + beta * MSELoss\n",
        "    '''\n",
        "    def __init__(self, alpha=1.0, beta=0.5):\n",
        "        super(MultiTaskLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "    \n",
        "    def forward(self, emotion_logits, va_pred, emotion_target, va_target):\n",
        "        # Classification loss\n",
        "        loss_emotion = self.ce_loss(emotion_logits, emotion_target)\n",
        "        \n",
        "        # Regression loss\n",
        "        loss_va = self.mse_loss(va_pred, va_target)\n",
        "        \n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * loss_emotion + self.beta * loss_va\n",
        "        \n",
        "        return total_loss, loss_emotion, loss_va\n",
        "\n",
        "# Initialize loss\n",
        "criterion = MultiTaskLoss(alpha=1.0, beta=0.5)\n",
        "print(f\"Loss weights - Alpha (Classification): {criterion.alpha}, Beta (Regression): {criterion.beta}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell10_header"
      },
      "source": [
        "### 10. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell10"
      },
      "outputs": [],
      "source": [
        "def calculate_ccc(y_true, y_pred):\n",
        "    '''\n",
        "    Calculate Concordance Correlation Coefficient (CCC)\n",
        "    Measures agreement between predicted and actual values\n",
        "    '''\n",
        "    mean_true = np.mean(y_true)\n",
        "    mean_pred = np.mean(y_pred)\n",
        "    var_true = np.var(y_true)\n",
        "    var_pred = np.var(y_pred)\n",
        "    covariance = np.mean((y_true - mean_true) * (y_pred - mean_pred))\n",
        "    \n",
        "    ccc = (2 * covariance) / (var_true + var_pred + (mean_true - mean_pred)**2 + 1e-8)\n",
        "    return ccc\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    '''\n",
        "    Comprehensive model evaluation\n",
        "    \n",
        "    Returns:\n",
        "        Dict with classification and regression metrics\n",
        "    '''\n",
        "    model.eval()\n",
        "    \n",
        "    # Storage for predictions and targets\n",
        "    all_emotion_preds = []\n",
        "    all_emotion_targets = []\n",
        "    all_va_preds = []\n",
        "    all_va_targets = []\n",
        "    \n",
        "    total_loss = 0.0\n",
        "    total_emotion_loss = 0.0\n",
        "    total_va_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, emotions, va_targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            emotions = emotions.to(device)\n",
        "            va_targets = va_targets.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            emotion_logits, va_preds = model(images)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss, emotion_loss, va_loss = criterion(emotion_logits, va_preds, emotions, va_targets)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_emotion_loss += emotion_loss.item()\n",
        "            total_va_loss += va_loss.item()\n",
        "            \n",
        "            # Get predictions\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim=1)\n",
        "            \n",
        "            # Store predictions and targets\n",
        "            all_emotion_preds.extend(emotion_preds.cpu().numpy())\n",
        "            all_emotion_targets.extend(emotions.cpu().numpy())\n",
        "            all_va_preds.extend(va_preds.cpu().numpy())\n",
        "            all_va_targets.extend(va_targets.cpu().numpy())\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    all_emotion_preds = np.array(all_emotion_preds)\n",
        "    all_emotion_targets = np.array(all_emotion_targets)\n",
        "    all_va_preds = np.array(all_va_preds)\n",
        "    all_va_targets = np.array(all_va_targets)\n",
        "    \n",
        "    # Calculate classification metrics\n",
        "    accuracy = np.mean(all_emotion_preds == all_emotion_targets)\n",
        "    macro_f1 = f1_score(all_emotion_targets, all_emotion_preds, average='macro')\n",
        "    \n",
        "    # Calculate regression metrics\n",
        "    va_mse = mean_squared_error(all_va_targets, all_va_preds)\n",
        "    va_mae = mean_absolute_error(all_va_targets, all_va_preds)\n",
        "    \n",
        "    # Separate valence and arousal\n",
        "    valence_true = all_va_targets[:, 0]\n",
        "    valence_pred = all_va_preds[:, 0]\n",
        "    arousal_true = all_va_targets[:, 1]\n",
        "    arousal_pred = all_va_preds[:, 1]\n",
        "    \n",
        "    # Calculate CCC for valence and arousal\n",
        "    ccc_valence = calculate_ccc(valence_true, valence_pred)\n",
        "    ccc_arousal = calculate_ccc(arousal_true, arousal_pred)\n",
        "    mean_ccc = (ccc_valence + ccc_arousal) / 2\n",
        "    \n",
        "    # Pearson correlation\n",
        "    pearson_valence, _ = pearsonr(valence_true, valence_pred)\n",
        "    pearson_arousal, _ = pearsonr(arousal_true, arousal_pred)\n",
        "    \n",
        "    metrics = {\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'emotion_loss': total_emotion_loss / len(dataloader),\n",
        "        'va_loss': total_va_loss / len(dataloader),\n",
        "        'accuracy': accuracy,\n",
        "        'macro_f1': macro_f1,\n",
        "        'va_mse': va_mse,\n",
        "        'va_mae': va_mae,\n",
        "        'ccc_valence': ccc_valence,\n",
        "        'ccc_arousal': ccc_arousal,\n",
        "        'mean_ccc': mean_ccc,\n",
        "        'pearson_valence': pearson_valence,\n",
        "        'pearson_arousal': pearson_arousal,\n",
        "        'confusion_matrix': confusion_matrix(all_emotion_targets, all_emotion_preds)\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"Evaluation functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell11_header"
      },
      "source": [
        "### 11. Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell11"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "PATIENCE = 10  # Early stopping patience\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_f1': [],\n",
        "    'val_ccc': []\n",
        "}\n",
        "\n",
        "# Best model tracking\n",
        "best_f1 = 0.0\n",
        "best_ccc = 0.0\n",
        "best_combined_score = 0.0  # f1 + ccc\n",
        "epochs_no_improve = 0\n",
        "\n",
        "print(f\"Training Configuration:\")\n",
        "print(f\"- Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"- Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"- Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"- Optimizer: Adam\")\n",
        "print(f\"- Scheduler: ReduceLROnPlateau\")\n",
        "print(f\"- Early Stopping Patience: {PATIENCE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell12_header"
      },
      "source": [
        "### 12. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell12"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # ========== TRAINING ==========\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    \n",
        "    for batch_idx, (images, emotions, va_targets) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        emotions = emotions.to(device)\n",
        "        va_targets = va_targets.to(device)\n",
        "        \n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        emotion_logits, va_preds = model(images)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss, emotion_loss, va_loss = criterion(emotion_logits, va_preds, emotions, va_targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track metrics\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.argmax(emotion_logits, dim=1)\n",
        "        train_correct += (preds == emotions).sum().item()\n",
        "        train_total += emotions.size(0)\n",
        "        \n",
        "        # Print progress\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Batch [{batch_idx+1}/{len(train_loader)}] \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "    \n",
        "    # Calculate training metrics\n",
        "    train_loss_avg = train_loss / len(train_loader)\n",
        "    train_acc = train_correct / train_total\n",
        "    \n",
        "    # ========== VALIDATION ==========\n",
        "    val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Update history\n",
        "    history['train_loss'].append(train_loss_avg)\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_metrics['accuracy'])\n",
        "    history['val_f1'].append(val_metrics['macro_f1'])\n",
        "    history['val_ccc'].append(val_metrics['mean_ccc'])\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_metrics['macro_f1'])\n",
        "    \n",
        "    # Print epoch summary\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - Time: {epoch_time:.2f}s\")\n",
        "    print(f\"Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Val Macro F1: {val_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"Val CCC - Valence: {val_metrics['ccc_valence']:.4f} | Arousal: {val_metrics['ccc_arousal']:.4f} | Mean: {val_metrics['mean_ccc']:.4f}\")\n",
        "    print(f\"Val MSE: {val_metrics['va_mse']:.4f} | Val MAE: {val_metrics['va_mae']:.4f}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Save best model\n",
        "    combined_score = val_metrics['macro_f1'] + val_metrics['mean_ccc']\n",
        "    \n",
        "    if combined_score > best_combined_score:\n",
        "        best_combined_score = combined_score\n",
        "        best_f1 = val_metrics['macro_f1']\n",
        "        best_ccc = val_metrics['mean_ccc']\n",
        "        epochs_no_improve = 0\n",
        "        \n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_f1': val_metrics['macro_f1'],\n",
        "            'val_ccc': val_metrics['mean_ccc'],\n",
        "        }, '/content/drive/MyDrive/best_model.pth')\n",
        "        \n",
        "        print(f\"✓ Best model saved! F1: {best_f1:.4f}, CCC: {best_ccc:.4f}\\n\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement for {epochs_no_improve} epoch(s)\\n\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Best Validation F1: {best_f1:.4f}\")\n",
        "print(f\"Best Validation CCC: {best_ccc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell13_header"
      },
      "source": [
        "### 13. Plot History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell13"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
        "axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Training and Validation Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 1].plot(history['train_acc'], label='Train Accuracy')\n",
        "axes[0, 1].plot(history['val_acc'], label='Val Accuracy')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Training and Validation Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# Macro F1\n",
        "axes[1, 0].plot(history['val_f1'], label='Val Macro F1', color='green')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Macro F1')\n",
        "axes[1, 0].set_title('Validation Macro F1 Score')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# Mean CCC\n",
        "axes[1, 1].plot(history['val_ccc'], label='Val Mean CCC', color='orange')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Mean CCC')\n",
        "axes[1, 1].set_title('Validation Mean CCC')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell14_header"
      },
      "source": [
        "### 14. Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell14"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('/content/drive/MyDrive/best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(\"Best model loaded!\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_metrics = evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
        "print(f\"\\nClassification Metrics:\")\n",
        "print(f\"  - Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  - Macro F1: {test_metrics['macro_f1']:.4f}\")\n",
        "print(f\"\\nRegression Metrics:\")\n",
        "print(f\"  - MSE: {test_metrics['va_mse']:.4f}\")\n",
        "print(f\"  - MAE: {test_metrics['va_mae']:.4f}\")\n",
        "print(f\"  - CCC Valence: {test_metrics['ccc_valence']:.4f}\")\n",
        "print(f\"  - CCC Arousal: {test_metrics['ccc_arousal']:.4f}\")\n",
        "print(f\"  - Mean CCC: {test_metrics['mean_ccc']:.4f}\")\n",
        "print(f\"  - Pearson Valence: {test_metrics['pearson_valence']:.4f}\")\n",
        "print(f\"  - Pearson Arousal: {test_metrics['pearson_arousal']:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell15_header"
      },
      "source": [
        "### 15. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell15"
      },
      "outputs": [],
      "source": [
        "# Emotion labels\n",
        "emotion_labels = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(test_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell16_header"
      },
      "source": [
        "### 16. Export: PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell16"
      },
      "outputs": [],
      "source": [
        "# Already saved during training as best_model.pth\n",
        "# Create a clean export with just model weights\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/emotion_model_weights.pth')\n",
        "print(\"✓ PyTorch model weights saved: emotion_model_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell17_header"
      },
      "source": [
        "### 17. Export: TorchScript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell17"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create example input\n",
        "example_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "# Trace the model\n",
        "traced_model = torch.jit.trace(model, example_input)\n",
        "\n",
        "# Save TorchScript model\n",
        "traced_model.save('/content/drive/MyDrive/emotion_model_torchscript.pt')\n",
        "print(\"✓ TorchScript model saved: emotion_model_torchscript.pt\")\n",
        "\n",
        "# Test loading\n",
        "loaded_ts = torch.jit.load('/content/drive/MyDrive/emotion_model_torchscript.pt')\n",
        "loaded_ts.eval()\n",
        "print(\"✓ TorchScript model verified!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell18_header"
      },
      "source": [
        "### 18. Export: ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell18"
      },
      "outputs": [],
      "source": [
        "import torch.onnx\n",
        "\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "\n",
        "# Example input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    '/content/drive/MyDrive/emotion_model.onnx',\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['emotion_logits', 'va_output'],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'emotion_logits': {0: 'batch_size'},\n",
        "        'va_output': {0: 'batch_size'}\n",
        "    }\n",
        ")\n",
        "print(\"✓ ONNX model saved: emotion_model.onnx\")\n",
        "\n",
        "# Verify ONNX model\n",
        "import onnx\n",
        "onnx_model = onnx.load('/content/drive/MyDrive/emotion_model.onnx')\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"✓ ONNX model verified!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell19_header"
      },
      "source": [
        "### 19. Export: Keras H5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell19"
      },
      "outputs": [],
      "source": [
        "# Note: Converting PyTorch to Keras requires intermediate ONNX conversion\n",
        "# We'll use onnx2keras or manually reconstruct in TensorFlow\n",
        "\n",
        "# Method 1: Using onnx2keras (install if needed)\n",
        "# !pip install onnx2keras tensorflow\n",
        "\n",
        "try:\n",
        "    import onnx\n",
        "    from onnx2keras import onnx_to_keras\n",
        "    import tensorflow as tf\n",
        "    \n",
        "    # Load ONNX model\n",
        "    onnx_model = onnx.load('/content/drive/MyDrive/emotion_model.onnx')\n",
        "    \n",
        "    # Convert to Keras\n",
        "    keras_model = onnx_to_keras(onnx_model, ['input'])\n",
        "    \n",
        "    # Save as H5\n",
        "    keras_model.save('/content/drive/MyDrive/emotion_model.h5')\n",
        "    print(\"✓ Keras H5 model saved: emotion_model.h5\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Warning: H5 export failed: {e}\")\n",
        "    print(\"Alternative: Use TensorFlow to manually reconstruct the model\")\n",
        "    \n",
        "    # Alternative: Create equivalent Keras model\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    \n",
        "    # Define equivalent Keras model\n",
        "    def create_keras_emotion_model():\n",
        "        # Base ResNet18 (pretrained weights need separate loading)\n",
        "        base_model = tf.keras.applications.ResNet50(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(224, 224, 3),\n",
        "            pooling='avg'\n",
        "        )\n",
        "        \n",
        "        # Freeze base layers\n",
        "        base_model.trainable = False\n",
        "        \n",
        "        # Input\n",
        "        inputs = keras.Input(shape=(224, 224, 3))\n",
        "        \n",
        "        # Shared features\n",
        "        x = base_model(inputs)\n",
        "        \n",
        "        # Emotion classification branch\n",
        "        emotion_branch = keras.layers.Dropout(0.5)(x)\n",
        "        emotion_branch = keras.layers.Dense(256, activation='relu')(emotion_branch)\n",
        "        emotion_branch = keras.layers.Dropout(0.3)(emotion_branch)\n",
        "        emotion_logits = keras.layers.Dense(8, name='emotion_logits')(emotion_branch)\n",
        "        \n",
        "        # VA regression branch\n",
        "        va_branch = keras.layers.Dropout(0.5)(x)\n",
        "        va_branch = keras.layers.Dense(128, activation='relu')(va_branch)\n",
        "        va_branch = keras.layers.Dropout(0.3)(va_branch)\n",
        "        va_output = keras.layers.Dense(2, activation='tanh', name='va_output')(va_branch)\n",
        "        \n",
        "        # Create model\n",
        "        model = keras.Model(inputs=inputs, outputs=[emotion_logits, va_output])\n",
        "        return model\n",
        "    \n",
        "    keras_model = create_keras_emotion_model()\n",
        "    keras_model.save('/content/drive/MyDrive/emotion_model_keras.h5')\n",
        "    print(\"✓ Keras H5 model (architecture) saved: emotion_model_keras.h5\")\n",
        "    print(\"Note: Weights need to be transferred separately from PyTorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell20_header"
      },
      "source": [
        "### 20. Save Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell20"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Model configuration\n",
        "config = {\n",
        "    'model_name': 'MultiTaskEmotionNet',\n",
        "    'architecture': 'ResNet18',\n",
        "    'num_classes': 8,\n",
        "    'emotion_labels': ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt'],\n",
        "    'input_size': [224, 224],\n",
        "    'normalization': {\n",
        "        'mean': [0.485, 0.456, 0.406],\n",
        "        'std': [0.229, 0.224, 0.225]\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'num_epochs': NUM_EPOCHS,\n",
        "        'optimizer': 'Adam',\n",
        "        'scheduler': 'ReduceLROnPlateau',\n",
        "        'loss_weights': {\n",
        "            'alpha': 1.0,\n",
        "            'beta': 0.5\n",
        "        }\n",
        "    },\n",
        "    'performance': {\n",
        "        'test_accuracy': float(test_metrics['accuracy']),\n",
        "        'test_macro_f1': float(test_metrics['macro_f1']),\n",
        "        'test_mean_ccc': float(test_metrics['mean_ccc']),\n",
        "        'test_mse': float(test_metrics['va_mse']),\n",
        "        'test_mae': float(test_metrics['va_mae'])\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save configuration\n",
        "with open('/content/drive/MyDrive/model_config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "print(\"✓ Model configuration saved: model_config.json\")\n",
        "print(\"\\nConfiguration:\")\n",
        "print(json.dumps(config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell21_header"
      },
      "source": [
        "### 21. Package & Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell21"
      },
      "outputs": [],
      "source": [
        "# Create a zip with all exported models\n",
        "!cd /content/drive/MyDrive && zip -r emotion_models.zip \\\n",
        "    best_model.pth \\\n",
        "    emotion_model_weights.pth \\\n",
        "    emotion_model_torchscript.pt \\\n",
        "    emotion_model.onnx \\\n",
        "    emotion_model.h5 \\\n",
        "    model_config.json \\\n",
        "    training_history.png \\\n",
        "    confusion_matrix.png\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nExported Files:\")\n",
        "print(\"1. best_model.pth - Full checkpoint with optimizer state\")\n",
        "print(\"2. emotion_model_weights.pth - Model weights only\")\n",
        "print(\"3. emotion_model_torchscript.pt - TorchScript format\")\n",
        "print(\"4. emotion_model.onnx - ONNX format\")\n",
        "print(\"5. emotion_model.h5 - Keras/H5 format\")\n",
        "print(\"6. model_config.json - Configuration file\")\n",
        "print(\"7. training_history.png - Training plots\")\n",
        "print(\"8. confusion_matrix.png - Confusion matrix\")\n",
        "print(\"\\nAll files saved to Google Drive!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
